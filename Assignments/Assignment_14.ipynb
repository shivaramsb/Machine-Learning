{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. Explain the concept of regularization in the context of linear regression.**"
      ],
      "metadata": {
        "id": "ozbVn62qfqgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Regularization is a technique used to prevent overfitting in linear regression models by adding a penalty term to the loss function. The two most common types of regularization are:\n",
        "\n",
        "* Lasso Regression (L1 regularization): Adds the absolute value of the coefficients as a penalty term to the loss function. It can shrink some coefficients to zero, effectively performing feature selection.\n",
        "Ridge Regression (L2 regularization): Adds the square of the coefficients as a penalty term to the loss function. It shrinks the coefficients but does not set them to zero.\n",
        "The regularization term helps to constrain the model complexity, promoting simpler models that generalize better to unseen data."
      ],
      "metadata": {
        "id": "gzFDEKo0fqlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.  What is logistic regression, and how does it differ from linear regression?**"
      ],
      "metadata": {
        "id": "PGBQB9QYfqpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is a supervised learning algorithm used for binary classification tasks. It models the probability that a given input belongs to a particular class by using the logistic function (sigmoid function). The output of logistic regression is a probability value between 0 and 1, which can be thresholded to make a binary decision. Unlike linear regression, which predicts a continuous target variable, logistic regression predicts a categorical outcome (0 or 1). The decision boundary in logistic regression is linear, but the predicted probabilities are transformed using the logistic function to lie within the [0, 1] range."
      ],
      "metadata": {
        "id": "4ebUsRCqfqsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.  What are decision trees, and how are they used for classification and regression?**"
      ],
      "metadata": {
        "id": "U35I4jDmfqwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Decision trees are a type of supervised learning algorithm used for both classification and regression tasks. They work by recursively splitting the data based on feature values to create a tree-like structure, where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents the final prediction (class label or continuous value).\n",
        "\n",
        "* Classification: Decision trees split the data based on feature values that maximize the separation between classes. The predicted class for a given input is determined by the majority class at the leaf node.\n",
        "Regression: Decision trees split the data based on feature values that minimize the variance within the resulting subsets. The predicted value for a given input is the average value at the leaf node.\n",
        "Decision trees are easy to interpret and visualize but can be prone to overfitting if not properly pruned or regularized."
      ],
      "metadata": {
        "id": "goRM3ViUfqzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Explain the concept of overfitting and how it can be mitigated in decision trees.**"
      ],
      "metadata": {
        "id": "Odqj2q9HgYKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Overfitting occurs when a decision tree model learns the noise and details of the training data to the extent that it performs poorly on new, unseen data. This happens when the tree becomes too complex, with too many nodes and branches. Overfitting can be mitigated in decision trees by:\n",
        "\n",
        "* Pruning: Reducing the size of the tree by removing nodes that provide little to no improvement in predictive performance. This can be done through techniques like cost-complexity pruning.\n",
        "Setting a maximum depth: Limiting the depth of the tree to prevent it from growing too complex.\n",
        "Minimum samples per leaf: Setting a minimum number of samples required to form a leaf node to avoid splitting on small, potentially noisy subsets.\n",
        "* Using ensemble methods: Combining multiple decision trees through techniques like bagging (e.g., Random Forests) or boosting (e.g., Gradient Boosting) to improve generalization."
      ],
      "metadata": {
        "id": "BXenlsftgYQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.What is ensemble learning, and how does it improve model performance?**"
      ],
      "metadata": {
        "id": "UQwjselDgYUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Ensemble learning is a technique that combines multiple base models (often called weak learners) to create a more robust and accurate final model (ensemble). The idea is that by aggregating the predictions of multiple models, the ensemble can reduce the variance, bias, or improve the generalization performance. There are several ensemble methods, including:\n",
        "\n",
        "* Bagging (Bootstrap Aggregating): Builds multiple models on different subsets of the training data (created by bootstrapping) and averages their predictions (e.g., Random Forests).\n",
        "Boosting: Sequentially builds models, each one focusing on the errors of the previous model, and combines their predictions with weighted voting (e.g., AdaBoost, Gradient Boosting).\n",
        "Ensemble learning improves model performance by leveraging the strengths of individual models and reducing the impact of their weaknesses."
      ],
      "metadata": {
        "id": "kuxmiu6cgYb-"
      }
    }
  ]
}