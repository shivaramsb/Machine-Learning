{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is dimensionality reduction, and why is it important?**"
      ],
      "metadata": {
        "id": "ozbVn62qfqgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is the process of reducing the number of features (dimensions) in a dataset while preserving as much information as possible. It is important because it:\n",
        "\n",
        "Helps mitigate the curse of dimensionality, which can degrade the performance of machine learning algorithms.\n",
        "Reduces computational complexity and storage requirements.\n",
        "Helps in visualizing high-dimensional data in lower dimensions.\n",
        "Common dimensionality reduction techniques include Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE), and Linear Discriminant Analysis (LDA)."
      ],
      "metadata": {
        "id": "gzFDEKo0fqlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.  Describe the t-SNE algorithm and its applications.**"
      ],
      "metadata": {
        "id": "PGBQB9QYfqpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " t-Distributed Stochastic Neighbor Embedding (t-SNE) is a dimensionality reduction technique particularly suited for visualizing high-dimensional data in two or three dimensions. It works by:\n",
        "\n",
        "Converting the high-dimensional Euclidean distances between data points into conditional probabilities representing similarities.\n",
        "Creating a similar probability distribution in a lower-dimensional space.\n",
        "Minimizing the Kullback-Leibler divergence between the two probability distributions using gradient descent.\n",
        "t-SNE is widely used for visualizing the structure of high-dimensional data, such as in gene expression data, image datasets, and word embeddings."
      ],
      "metadata": {
        "id": "4ebUsRCqfqsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.  What is anomaly detection, and how is it related to unsupervised learning?**"
      ],
      "metadata": {
        "id": "U35I4jDmfqwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anomaly detection is the process of identifying data points that deviate significantly from the majority of the data, indicating potential anomalies or outliers. It is often treated as an unsupervised learning problem because anomalies are rare and labeled data is typically unavailable. Techniques for anomaly detection include clustering-based methods, density-based methods (e.g., DBSCAN), statistical methods, and autoencoders. Applications include fraud detection, network security, and fault detection in industrial systems."
      ],
      "metadata": {
        "id": "goRM3ViUfqzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Explain the concept of an autoencoder.**"
      ],
      "metadata": {
        "id": "Odqj2q9HgYKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An autoencoder is a type of neural network used for unsupervised learning, particularly for dimensionality reduction and feature learning. It consists of two main parts:\n",
        "\n",
        "Encoder: Maps the input data to a lower-dimensional latent space.\n",
        "Decoder: Reconstructs the input data from the lower-dimensional representation.\n",
        "The network is trained to minimize the reconstruction error between the original input and the reconstructed output. Autoencoders are used for applications such as denoising, anomaly detection, and data compression."
      ],
      "metadata": {
        "id": "BXenlsftgYQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Describe the Gaussian Mixture Model (GMM) and its applications.**"
      ],
      "metadata": {
        "id": "UQwjselDgYUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Gaussian Mixture Model (GMM) is a probabilistic model used for clustering that assumes the data is generated from a mixture of several Gaussian distributions with unknown parameters. Each component in the mixture represents a cluster, and the model estimates the mean, covariance, and mixing coefficient of each Gaussian component. The steps involved in GMM are:\n",
        "\n",
        "Initialize the parameters of the Gaussian components.\n",
        "Use the Expectation-Maximization (EM) algorithm to iteratively update the parameters until convergence.\n",
        "Assign data points to clusters based on the highest posterior probability.\n",
        "GMM is used in applications such as image segmentation, speaker recognition, and density estimation."
      ],
      "metadata": {
        "id": "kuxmiu6cgYb-"
      }
    }
  ]
}