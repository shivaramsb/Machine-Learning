{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is data preprocessing, and why is it important in machine learning?**"
      ],
      "metadata": {
        "id": "ozbVn62qfqgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data preprocessing involves transforming raw data into a clean and suitable format for analysis. It is a crucial step in the machine learning pipeline because real-world data is often incomplete, inconsistent, and noisy. Preprocessing helps to improve the quality of the data, making it more conducive for building robust and accurate machine learning models. It includes tasks like data cleaning, normalization, transformation, and feature engineering."
      ],
      "metadata": {
        "id": "gzFDEKo0fqlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Describe three common techniques for handling missing data.**"
      ],
      "metadata": {
        "id": "PGBQB9QYfqpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Imputation: Replacing missing values with a statistical measure such as mean, median, or mode. For example, if a dataset of ages has missing values, one might replace the missing values with the mean age.\n",
        "* Deletion: Removing rows or columns with missing values. This technique is simple but can lead to significant data loss if many values are missing.\n",
        "* Using Algorithms that Support Missing Values: Some machine learning algorithms, such as certain tree-based models, can handle missing values directly without the need for imputation or deletion."
      ],
      "metadata": {
        "id": "4ebUsRCqfqsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.  What is feature scaling, and why is it necessary?**"
      ],
      "metadata": {
        "id": "U35I4jDmfqwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling involves adjusting the scales of features so that they have comparable magnitudes. This is necessary because many machine learning algorithms, such as gradient descent-based methods, are sensitive to the scale of input features. Without scaling, features with larger magnitudes can dominate the learning process, leading to poor model performance. Common techniques for feature scaling include:\n",
        "\n",
        "* Normalization: Rescaling features to a range of [0, 1] or [-1, 1].\n",
        "* Standardization: Rescaling features to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "goRM3ViUfqzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Explain the concept of one-hot encoding. When is it used?**"
      ],
      "metadata": {
        "id": "Odqj2q9HgYKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is a technique used to convert categorical variables into a binary (0 or 1) matrix representation. Each category in the original variable is represented as a binary vector with only one '1' and the rest '0's. It is used when categorical variables have no ordinal relationship and should be treated as separate entities. For example, if a categorical variable \"color\" has values \"red,\" \"blue,\" and \"green,\" one-hot encoding would create three binary variables representing each color."
      ],
      "metadata": {
        "id": "BXenlsftgYQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. What is the purpose of data normalization, and how is it different from standardization?**"
      ],
      "metadata": {
        "id": "UQwjselDgYUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of data normalization is to rescale the features to a standard range, typically [0, 1] or [-1, 1], to ensure that no single feature dominates others due to its scale. It is different from standardization in that normalization adjusts the range of the data, while standardization adjusts the distribution to have a mean of 0 and a standard deviation of 1. Normalization is particularly useful for algorithms that rely on distances or gradients, such as k-nearest neighbors or neural networks.\n",
        "\n"
      ],
      "metadata": {
        "id": "kuxmiu6cgYb-"
      }
    }
  ]
}