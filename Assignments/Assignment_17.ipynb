{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is the confusion matrix, and how is it used in evaluating the performance of a classification model?**"
      ],
      "metadata": {
        "id": "ozbVn62qfqgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix is a table that describes the performance of a classification model by comparing the actual and predicted labels. It contains the following four metrics:\n",
        "\n",
        "True Positives (TP): Correct positive predictions.\n",
        "True Negatives (TN): Correct negative predictions.\n",
        "False Positives (FP): Incorrect positive predictions (Type I error).\n",
        "False Negatives (FN): Incorrect negative predictions (Type II error).\n",
        "The confusion matrix helps in calculating various performance metrics such as accuracy, precision, recall, and F1 score."
      ],
      "metadata": {
        "id": "gzFDEKo0fqlE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.  Define the ROC curve and the AUC. How are they used in evaluating model performance?**"
      ],
      "metadata": {
        "id": "PGBQB9QYfqpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ROC Curve: The Receiver Operating Characteristic (ROC) curve is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold settings. It shows the trade-off between sensitivity and specificity.\n",
        "AUC: The Area Under the ROC Curve (AUC) measures the overall performance of a classification model. It ranges from 0 to 1, where 1 indicates a perfect model, 0.5 indicates a model with no discriminative power (random guessing), and below 0.5 indicates a model performing worse than random guessing.\n",
        "AUC-ROC is used to compare different models and select the best one."
      ],
      "metadata": {
        "id": "4ebUsRCqfqsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.  What is the accuracy paradox, and why can accuracy be a misleading metric in some cases?**"
      ],
      "metadata": {
        "id": "U35I4jDmfqwP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy paradox occurs when a model with high accuracy performs poorly on important aspects of the data. This often happens in imbalanced datasets where the number of instances in one class far outweighs the other. In such cases, a model can achieve high accuracy by simply predicting the majority class most of the time, ignoring the minority class, which might be of greater interest."
      ],
      "metadata": {
        "id": "goRM3ViUfqzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What is the Matthews correlation coefficient (MCC), and how is it calculated?**"
      ],
      "metadata": {
        "id": "Odqj2q9HgYKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Matthews correlation coefficient (MCC) is a metric that takes into account true and false positives and negatives, providing a balanced measure even for imbalanced classes. It ranges from -1 to 1, where 1 indicates a perfect prediction, 0 indicates no better than random prediction, and -1 indicates total disagreement between prediction and actual."
      ],
      "metadata": {
        "id": "BXenlsftgYQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.How do precision-recall curves differ from ROC curves? When should you use precision-recall curves?**"
      ],
      "metadata": {
        "id": "UQwjselDgYUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision-recall curves plot precision against recall at various threshold settings. They are especially useful when dealing with imbalanced datasets where the positive class is of primary interest. Unlike ROC curves, which can be misleading in imbalanced datasets, precision-recall curves provide a clearer picture of the trade-off between precision and recall, helping to better assess the model's performance on the minority class."
      ],
      "metadata": {
        "id": "kuxmiu6cgYb-"
      }
    }
  ]
}